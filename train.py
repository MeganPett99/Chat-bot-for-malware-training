import json
from nltk_utils import tokenize, stem, stemmer
# inheritance of functions to be called
import numpy
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.optimizers import SGD
import pickle


with open('intents.json', 'r') as json_file:
    intents = json.load(json_file)
# opening the Json file as read mode and creating it as variable intents to load

words = []
labels = []
document = []
# words, classes and documents are empty lists

for intent in intents['intents']:
    tag = intent['tag']
    for pattern in intent['patterns']:
        w = tokenize(pattern)
        words.extend(w)
        document.append((w, intent["tag"]))
        if intent["tag"] not in labels:
            labels.append(intent["tag"])

# this is a nested for loop, the tag is tokenize and appended, the patten is called and tokenized and
# is then extended onto the words list
# then the Document list is then called and appended with both the tags
# tokenized patterns list to be used later in the training

ignore_words = ['?', '!', '.', ',']
words = [stem(w) for w in words if w not in ignore_words]
words = sorted(set(words))
labels = sorted(set(labels))
# the list are then sorted by set to stop it becoming random
pickle.dump(words, open('words.pkl', 'wb'))
pickle.dump(labels, open('labels.pkl', 'wb'))
# wb means write binary, use this later for bags of words

training = []
output_empty = [0] * len(labels)

for doc in document:
    bag = []
    token_words = doc[0]
    token_words = [stemmer.stem(w.lower()) for w in token_words]
    for w in words:
        if w in token_words:
            bag.append(1)
        else:
            bag.append(0)
    output_row = list(output_empty)
    output_row[labels.index(doc[1])] = 1
    training.append([bag, output_row])
# this nested for loop is creating the bag of word from the tokenized words in the document list
# the tokenized patterns are then stemmed
# the bag of words is created by appending the value 1 to words that are found in its current position in the document
# If a word isn't in a current position its appending with the value 0
# As a result output is 0 and 1 is for each tag(pattern)

training = numpy.array(training)
train_x = list(training[:, 0])
train_y = list(training[:, 1])
print("Training data created")
# The training list that was appended in the bag of list is then configured to a numpy array
# using the 0 and 1 means that train_x is the patterns and Y is the intents

model = Sequential()
model.add(Dense(100, input_shape=(len(train_x[0]),), activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(len(train_y[0]), activation='softmax'))
# input layer, one hidden layers, dropout set as 0.5 for minimum loss, activated softmax to help determine best response
sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
model.summary()
hist = model.fit(numpy.array(train_x), numpy.array(train_y), epochs=220, batch_size=5, verbose=1)
model.save('chat_model.h5', hist)
print("model created")
